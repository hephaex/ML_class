# 온수당 Machine Learing Note (week #8)
* Support Vector Machine
* progress: 8 주차
*
* date: 2016.07.11
* 장소: 

* Note:
 - 1주와 2주는 supervised learing 에서 Linear Regression(선형 회귀)을 공부했습니다.
 - 3주는 classification(군집화)에 대하여 공부했습니다.
 - 4주는 기계학습에서 사용하는 신경망(neural network)을 공부했습니다.
 - 5주는 신경망이 어떻게 학습을 하는지에 대하여 공부합니다.
 - 5주차 이후 중간 복습하는 시간을 가졌습니다. 
 - 6주차에서는 기계학습에 도움될 여러가지 것을 공부합니다. 
 - 6주차에서 Machine learning에 디자인에 대한 부분을 시간상 이번주에 하고,
 - 7주차에서 해야 할 Support Vector Machine에 대한 부분을 공부합니다.
 - 8주차에서는 Non-Supervisor Learning과 차원 축소 이론과 대표적인 차원 축소인 PCA를 공부합니다. 

# Contents
* Clustering
 - Unsupervised Learning: introduction
 - K-means Algorithm
 - Optimization Objective
 - Random Initialization
 - Choosing the Number of Clusters

* Dimensionality Reduction
 - Motivation
   - Motivation I: Data Compression
   - Motivation II: Visualization
 - Pricipal Component Analysis
   - Principal Component Analysis Problem Formulation
   - Principal Component Analysis Algorithm
 - Applying PCA
   - Reconstruction from compressed Representation
   - Choosing the Number of Pricipal Components
   - Advice for Applying PCA
# Clustering

## Unsupervised Learning: introduction
![01-01](https://github.com/hephaex/ML_class/blob/master/week8/week8_01_Clust_01.png)

## K-means Algorithm
## Optimization Objective
## Random Initialization
## Choosing the Number of Clusters

# Dimensionality Reduction
## Motivation

### Motivation I: Data Compression
### Motivation II: Visualization

## Pricipal Component Analysis
### Principal Component Analysis Problem Formulation
### Principal Component Analysis Algorithm

## Applying PCA
### Reconstruction from compressed Representation
### Choosing the Number of Pricipal Components
### Advice for Applying PCA


